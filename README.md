
# MNIST — مدل ساده با دقت پایین (آموزشی)

این مخزن شامل یک اسکریپت پایتون به نام `low_accuracy_mnist.py` است که **به‌صورت عمدی** یک مدل بسیار ساده روی دیتاست MNIST آموزش می‌دهد تا:
- جریان پایه‌ی یک پروژه یادگیری عمیق (Load → Train → Evaluate → Visualize) را نشان دهد،
- و در عین حال به‌دلیل معماری بسیار کوچک و محدودیت داده‌ی آموزشی، **دقت نهایی پایین**ی داشته باشد (برای اهداف آموزشی/آزمایشی).

> اگر به دنبال دقت بالاتر هستید، بخش «بهبود دقت» را ببینید.


## محتوای مخزن
- `low_accuracy_mnist.py` — کد اصلی آموزش و ارزیابی مدل ساده MNIST.
- `README.md` — همین فایل راهنما.


## پیش‌نیازها
Python 3.8+ توصیه می‌شود. کتابخانه‌های مورد نیاز:
```bash
pip install torch torchvision matplotlib
```
> **نکته:** روی macOS یا محیط‌های headless/سرور، backend پیش‌فرض Matplotlib ممکن است مشکل داشته باشد (کد از `TkAgg` استفاده می‌کند). اگر خطای backend گرفتید، بخش «مشکلات رایج» را ببینید.


## اجرای سریع (لوکال)
1) کلون/دانلود پروژه و ورود به پوشه:
```bash
git clone <YOUR-REPO-URL>
cd <YOUR-REPO-FOLDER>
```
2) نصب وابستگی‌ها:
```bash
pip install torch torchvision matplotlib
```
3) اجرا:
```bash
python low_accuracy_mnist.py
```
در اولین اجرا دیتاست MNIST به صورت خودکار در پوشه `data/` دانلود می‌شود. خروجی شامل **Train Acc** برای هر epoch و در پایان **Test Accuracy** و یک پنجره‌ی نمایش پیش‌بینی‌های نمونه است.


## اجرای سریع (Google Colab)
- فایل `low_accuracy_mnist.py` را آپلود کنید یا محتوا را در یک سلول Colab کپی کنید.
- نصب پیش‌نیازها:
```python
!pip install torch torchvision matplotlib
```
- برای Colab (محیط headless) بهتر است خط زیر را در اسکریپت **کامنت** کنید یا به `Agg` تغییر دهید:
```python
# matplotlib.use("TkAgg")   # در Colab: این خط را حذف/کامنت کنید
```
- سپس اسکریپت را اجرا کنید. برای دیدن شکل به جای `plt.show()` می‌توانید از `plt.savefig("preds.png", dpi=150)` استفاده کنید.


## ساختار/منطق کد
- **مدل:** `SimpleDigitNet` با تنها یک لایه‌ی مخفی 5-نودی → عمداً کوچک و کم‌توان.
- **داده‌ها:** روی 6000 نمونه‌ی اولِ train (به‌صورت زیرمجموعه) آموزش می‌دهد تا خروجی ضعیف حاصل شود.
- **بهینه‌ساز:** `SGD` با `lr=0.005`.
- **نمایش:** تابع `show_predictions` چند نمونه از پیش‌بینی‌ها را نمایش می‌دهد.

> این تنظیمات عمداً برای «دقت پایین» انتخاب شده‌اند تا تفاوت با تنظیمات قوی‌تر به‌خوبی دیده شود.


## نتایج مورد انتظار
- دقت آموزش و تست بسته به سخت‌افزار/نسخه کتابخانه‌ها کمی متفاوت است، اما انتظار نداشته باشید که به دقت‌های رایج (>97%) برسد؛ این پروژه **آموزشی** است.


## بهبود دقت (اختیاری)
اگر می‌خواهید همان کد را به‌تدریج قوی‌تر کنید، می‌توانید تغییرات زیر را مرحله‌ای اعمال کنید:
1) **بزرگ‌تر کردن مدل:**
   - افزایش اندازه لایه‌ی مخفی (مثلاً از 5 به 128 یا 256).
   - افزودن یک یا دو لایه‌ی Fully-Connected دیگر.
2) **دیتای بیشتر:** به‌جای زیرمجموعه 6000تایی، کل MNIST train را استفاده کنید.
3) **Optimizer بهتر:** `Adam` با `lr=1e-3`.
4) **Epoch بیشتر:** از 10 به 20–30 افزایش دهید، همراه با Early Stopping.
5) **معماری کانولوشنی:** به‌جای MLP ساده، از CNNهای کم‌حجم (Conv2d/MaxPool) استفاده کنید.
6) **افزودن Dropout/BatchNorm** برای پایداری بیشتر.
7) **تنظیم نرخ یادگیری** (LR Scheduling).

نمونه‌ی کوچک برای تغییر اندازه‌ی لایه:
```python
self.layer1 = nn.Linear(28*28, 128)
self.output = nn.Linear(128, 10)
```


## مشکلات رایج و راه‌حل‌ها
- **ارور مربوط به Matplotlib backend (TkAgg):**
  - روی Colab یا سرورها، `TkAgg` معمولاً در دسترس نیست. راه‌حل‌ها:
    - خط `matplotlib.use("TkAgg")` را کامنت کنید؛
    - یا آن را به `Agg` تغییر دهید:
      ```python
      matplotlib.use("Agg")
      ```
    - سپس به‌جای `plt.show()` از `plt.savefig("preds.png", dpi=150)` استفاده کنید.
- **CUDA در دسترس نیست:** کد به‌طور خودکار روی CPU می‌افتد. برای سرعت بالاتر، CUDA را روی ماشین سازگار نصب/فعال کنید.


## سفارشی‌سازی خروجی برای گزارش/ارائه
- چاپ دقیق‌تر لاگ‌ها (Loss میانگین هر epoch، زمان هر epoch).
- ذخیره‌ی Checkpoint بهترین مدل:
```python
torch.save(model.state_dict(), "mnist_simple.pt")
```
- ذخیره‌ی تصویر پیش‌بینی‌ها:
```python
plt.savefig("preds.png", dpi=150)
```


## لایسنس
MIT — در صورت نیاز، متن لایسنس را در فایل `LICENSE` اضافه کنید.


## استناد (Citation)
در صورت استفاده آموزشی/پژوهشی، ارجاع به این ریپو/فایل کفایت می‌کند. اگر از MNIST استفاده می‌کنید، به مقاله‌ی LeCun et al. (1998) ارجاع دهید.
